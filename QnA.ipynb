{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99eb38f2",
   "metadata": {},
   "source": [
    "### Trying out Google PaLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a27fed1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install google-generativeai\n",
    "# pip install -r requirements.txt\n",
    "# conda install -c conda-forge sentence-transformers\n",
    "\n",
    "from langchain.llms import GooglePalm\n",
    "\n",
    "api_key = \"AIzaSyAPHVmjjz9KehLtMIxCXnhn90kSNmyN6eA\"\n",
    "llm = GooglePalm(google_api_key = api_key, temperature = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8091aa04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Travel the world**\n",
      "**See new places, meet new people**\n",
      "**Expand your horizons**\n"
     ]
    }
   ],
   "source": [
    "poem = llm(\"Write a haiku about travel\")\n",
    "print(poem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ef7b47",
   "metadata": {},
   "source": [
    "### Loading the CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da852d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path = \"codebasics_faqs.csv\", source_column = \"prompt\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85e89951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='prompt: I have never done programming in my life. Can I take this bootcamp?\\nresponse: Yes, this is the perfect bootcamp for anyone who has never done coding and wants to build a career in the IT/Data Analytics industry or just wants to perform better in your current job or business using data.', metadata={'source': 'I have never done programming in my life. Can I take this bootcamp?', 'row': 0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e18ad7b",
   "metadata": {},
   "source": [
    "### Creating Embeddings for the data\n",
    "\n",
    "- We are using the Instructor Embeddings from HuggingFace (https://instructor-embedding.github.io/)\n",
    "- This is a unique embedding library because unlike others, this is a single embedder that can generate text embeddings tailored to different downstream tasks and domains, without any further training. \n",
    "- Guide to embedding: https://huggingface.co/hkunlp/instructor-large\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67a6fe3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-0.04209407791495323,\n",
       " 0.008169054053723812,\n",
       " 0.0003134481084998697,\n",
       " 0.019235705956816673,\n",
       " 0.03759319335222244]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding sample example\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceInstructEmbeddings(query_instruction = \"Represent the query for retrieval: \")\n",
    "e = embeddings.embed_query(\"What is your refund policy?\")\n",
    "\n",
    "print(len(e))\n",
    "e[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2122cc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "# Embedding for the project\n",
    "# Initialize instructor embeddings using the Hugging Face model\n",
    "instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea43b5b2",
   "metadata": {},
   "source": [
    "Embedding for a sentance \"What is your refund policy\" is a list of size 768. Looking at the numbers in this list, doesn't give any intuitive understanding of what it is but just assume that these numbers are capturing the meaning of \"What is your refund policy\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f918c192",
   "metadata": {},
   "source": [
    "### Vector Store using FAISS\n",
    "\n",
    "- Usually, people store the data completely in the vector DB\n",
    "- But to make it simpler, we'll use it in-memory via the retriever object\n",
    "- When there is a new question, embeddings are created and then the retriever will find similar embedding and return the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07543d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Create a FAISS instance for vector database from 'data'\n",
    "# This creates a vector DB!\n",
    "vectordb = FAISS.from_documents(documents = data,\n",
    "                                 embedding = instructor_embeddings)\n",
    "\n",
    "# Create a retriever for querying the vector database\n",
    "retriever = vectordb.as_retriever(score_threshold = 0.7)\n",
    "\n",
    "rdocs = retriever.get_relevant_documents(\"For how long is this course valid?\")\n",
    "rdocs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2576e748",
   "metadata": {},
   "source": [
    "### Create RetrievalQA chain along with prompt template\n",
    "\n",
    " - \"context\" here is the CSV info we've given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b44d206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Given the following context and a question, generate an answer based on this context only.\n",
    "In the answer try to provide as much text as possible from \"response\" section in the source document context without making much changes.\n",
    "If the answer is not found in the context, kindly state \"I don't know.\" Don't try to make up an answer.\n",
    "\n",
    "CONTEXT: {context}\n",
    "\n",
    "QUESTION: {question}\"\"\"\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                            chain_type=\"stuff\",\n",
    "                            retriever=retriever,\n",
    "                            input_key=\"query\",\n",
    "                            return_source_documents=True,\n",
    "                            chain_type_kwargs=chain_type_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed3ce81",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain(\"Do you guys provide internship and also do you offer EMI payments?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2187f5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
